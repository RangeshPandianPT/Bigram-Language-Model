# Llama-from-Scratch

A pedagogical implementation of a modern Large Language Model (LLM) architecture, evolving from a simple GPT-2 style baseline to a Llama 2/3 style model.

## ğŸš€ Project Goal
To understand modern LLM internals by building them from scratch in PyTorch. This project bridges the gap between basic transformer tutorials (like Andrej Karpathy's `nanogpt`) and production-grade architectures like Llama 3.

## âœ¨ Key Features (Implemented)

### ğŸ—ï¸ Architecture ("Llama-fication")
- **RMSNorm** (Root Mean Square Normalization): Replaces LayerNorm for better stability and performance.
- **RoPE** (Rotary Positional Embeddings): Replaces absolute positional embeddings for better generalization to longer sequences.
- **SwiGLU** (SiLU Gated Linear Unit): A more powerful activation function in the FeedForward network.
- **Grouped Query Attention (GQA)**: Optimizes inference memory by sharing Key/Value heads across multiple Query heads (Llama 2/3 style).

### ğŸ› ï¸ Engineering
- **Modular Codebase**: Separated into `config.py`, `model.py`, `train.py`, `tokenizer.py`.
- **Mixed Precision Training**: Implemented via `torch.amp` for faster training on modern GPUs.
- **KV Cache**: Efficient O(N) generation inference.
- **Efficient Data Loading**: Uses `numpy.memmap` for handling datasets larger than RAM.

### ğŸ§  Tokenization
- Uses a custom BPE (Byte Pair Encoding) tokenizer trained on the dataset.

## ğŸ“‚ File Structure

| File | Purpose |
| --- | --- |
| `model.py` | Core Transformer: RMSNorm, RoPE, GQA, SwiGLU, KV Cache |
| `train.py` | Training loop with AMP, LR scheduling, gradient clipping |
| `config.py` | GPTConfig, TrainConfig, SamplingConfig dataclasses |
| `generate.py` | CLI inference with advanced sampling |
| `app.py` | ğŸŒ Gradio web demo |
| `tokenizer.py` | BPE tokenizer wrapper |
| `prepare_data.py` | Tokenizes raw text â†’ `train.bin` / `val.bin` |
| `test_gqa.py` | GQA unit tests (KV param reduction + output shape) |

## âš¡ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Prepare Data
```bash
python prepare_data.py
```

### 3. Train
```bash
python train.py
```

### 4. Generate Text (CLI)
```bash
python generate.py
```

## ğŸŒ Web Demo

Launch an interactive web interface to generate text with live parameter controls:

```bash
python app.py
```

Then open [http://localhost:7860](http://localhost:7860) in your browser.

**Demo controls:**

| Control | Description |
| --- | --- |
| ğŸŒ¡ï¸ Temperature | Creativity/randomness (0.1 = focused, 2.0 = wild) |
| ğŸ” Top-K | Keep only the top K most likely tokens |
| ğŸ¯ Top-P | Nucleus sampling â€” smallest set with cumulative prob â‰¥ P |
| ğŸ” Repetition Penalty | Discourages repeating tokens |
| ğŸ“ Max New Tokens | How many tokens to generate |

## ğŸ§ª Testing

```bash
python test_gqa.py        # GQA correctness: KV param reduction + output shape
python test_kv_cache.py   # KV Cache: verify cached == uncached outputs + speedup
python test_new_features.py  # Mixed precision, sampling strategies, LR scheduler
```

## ğŸ“œ License
MIT
